# Interapp

Interapp is a program that suggests user actions during a video call built on Machine Learning based on the emotion recognition of the image and audio of a video call.

The purposes of this project is to help user to notify the current emotion of the video calling subject and suggests them the action that could be taken to improve the mutual experiences of the video call. This is also useful for student that is taking video call as a type of interview, knowing the current emotion of the interviewer that initially could have gone unnoticed, might help student to avoid some pitfall of interview and thus getting a better result. 


# Overview

The structure of this project can roughly be separated into three part. 

## Image (Face Sentiment Analysis)


## Voice (Real-time Speech Emotion Recognizer)

## Action Suggestion (Reinforcement Learning using TensorFlow)



Lastly, having the ability to capture image and sound of any program on computer, its usage is not restricted on solely video call, but there is a vast possibility of it is capable of.
# Warps things up
Interapp would like to express our gratitude toward these awesome packages that make this project possible

- [Emopy](https://github.com/thoughtworksarts/EmoPy) -- A deep neural net toolkit for emotion analysis via Facial Expression Recognition (FER) by thoughtworksarts
- [Emovoice](https://github.com/hcmlab/emovoice) --Real-time Speech Emotion Recognizer by hcmlab
- [TensorFlow](https://www.tensorflow.org/tutorials/) --An open source machine learning framework for everyone
